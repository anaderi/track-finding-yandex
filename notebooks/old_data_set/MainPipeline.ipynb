{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building alternative pipeline\n",
    "\n",
    "* Prepare Hough transform based only on wire information (= energy, time, radial distance)\n",
    "* Build predictions based on Hough, local information, and local information of left/right neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%run visualizations.ipynb\n",
    "import sys\n",
    "sys.path.insert(0, '../modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hits import AllHits\n",
    "hits = AllHits('../data/signal_TDR.root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from tracking import Hough\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Collect (Raw) Wire Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right_neighs = numpy.array([hits.cydet.shift_wire(wire, 1) for wire in range(hits.cydet.n_points)])\n",
    "left_neighs = numpy.array([hits.cydet.shift_wire(wire, -1) for wire in range(hits.cydet.n_points)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Energy deposits\n",
    "deposits = numpy.vstack(hits.get_energy_deposits(i) for i in range(hits.n_events))\n",
    "deposits *= 100 # to avoid small numbers for GBRT\n",
    "# Time after triggering that signal waveforms starts\n",
    "rel_time = numpy.vstack(hits.get_relative_time(i) for i in range(hits.n_events))\n",
    "# Truth Values\n",
    "labels = numpy.vstack(hits.get_hit_types(i) for i in range(hits.n_events))\n",
    "# Layer ID of hit wires\n",
    "layer_id = numpy.vstack(hits.cydet.point_rhos for i in range(hits.n_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3445, 4482), (3445, 4482))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, deposits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Functions for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_data(data, keys, samples, n_features):\n",
    "    data_flat = data[samples].reshape(-1, n_feats)\n",
    "    return pandas.DataFrame(data=data_flat, columns=keys)\n",
    "\n",
    "def mask(data, pol=None):\n",
    "    if pol==None:\n",
    "        return data[data['labels'] != 0]\n",
    "    if pol==0:\n",
    "        return data[(data['labels'] != 0) & (data['Polarity'] == 0)]\n",
    "    if pol==1:\n",
    "        return data[(data['labels'] != 0) & (data['Polarity'] != 0)]\n",
    "\n",
    "def predict_classifier(classifier, data_samp):\n",
    "    \"\"\"\n",
    "    Return predictions where no hit has zero response by definition.  Return both\n",
    "    flat predictions, and event sorted predicitions\n",
    "    \"\"\"\n",
    "    # Get the predictions\n",
    "    pred_evt = classifier.predict_proba(data_samp)[:,1]\n",
    "\n",
    "    # Check which wires have hits, which ones do not\n",
    "    not_hit = (data_samp['labels'] == 0).values\n",
    "    has_hit = (data_samp['labels'] != 0).values\n",
    "\n",
    "    # Define hit-less wires as definitely not being signal\n",
    "    pred_evt[not_hit] = 0\n",
    "\n",
    "    # Slim down the result to a copy with only hits\n",
    "    pred = pred_evt[has_hit]\n",
    "    # Reshape the result to be the event shape, for the hoguh transform\n",
    "    pred_evt = pred_evt.reshape(-1, hits.cydet.n_points)\n",
    "    \n",
    "    return pred, pred_evt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Look up table to map from events to hits\n",
    "event_to_hit_look = np.arange(hits.n_events*hits.cydet.n_points).reshape(hits.n_events, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "rest, sample_3 = train_test_split(range(len(labels)), train_size=0.6, test_size=0.3)\n",
    "sample_1, sample_2 = train_test_split(rest, train_size=0.5, test_size=0.5)\n",
    "print (1704 in sample_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for Wire GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "features = OrderedDict()\n",
    "# Wire features\n",
    "features['deposit'] = deposits * 100\n",
    "features['rel_time'] = rel_time\n",
    "features['layer_id'] = layer_id\n",
    "\n",
    "# LR-Neighbour Features\n",
    "features['r_deposit'] = (deposits * 100)[:, right_neighs]\n",
    "features['r_rel_time'] = rel_time[:, right_neighs]\n",
    "features['l_deposit'] = (deposits * 100)[:, left_neighs]\n",
    "features['l_rel_time'] = rel_time[:, left_neighs]\n",
    "\n",
    "# Truth values\n",
    "features['labels'] = labels\n",
    "features['is_signal'] =  numpy.take([0, 1, 0], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3445, 4482, 9)\n"
     ]
    }
   ],
   "source": [
    "# Define Data\n",
    "train_features = features.keys()[:-2]\n",
    "data = numpy.dstack(features.values())\n",
    "n_feats = data.shape[-1]\n",
    "print data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_1, data_2, data_3 = [flatten_data(data, features.keys(), samp, n_feats) \n",
    "                          for samp in [sample_1, sample_2, sample_3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## NeighbourLevel GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rep.metaml.factory import ClassifiersFactory\n",
    "from rep.estimators import SklearnClassifier, TMVAClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from rep.report.metrics import RocAuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "factory = ClassifiersFactory()\n",
    "\n",
    "# Define GBDT over neighbour-level features\n",
    "neigh_gbdt = SklearnClassifier(GradientBoostingClassifier(n_estimators=100), \n",
    "                               features=train_features) \n",
    "# Add the GBDT\n",
    "factory.add_classifier('Local and Neighbour Features', neigh_gbdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model Local and Neighbour Features was trained in 122.37 seconds\n",
      "Totally spent 122.37 seconds on training\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "factory.fit(mask(data_1), mask(data_1)['is_signal'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print these predictions\n",
    "pred_2, pred_evt_2 = predict_classifier(neigh_gbdt, data_2)\n",
    "pred_3, pred_evt_3 = predict_classifier(neigh_gbdt, data_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing hough  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define which wires we want to correct as hits which were missclassified \n",
    "problem_weights_2 = (mask(data_2)['labels'] != 0) * abs(mask(data_2)['is_signal'] - pred_2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of classification, reweighted to highlight mislabelled hits\n",
    "Second scoring is used later to measure quality of Hough transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997102234067\n",
      "0.28784056483\n"
     ]
    }
   ],
   "source": [
    "print roc_auc_score(mask(data_2)['is_signal'], pred_2)\n",
    "print roc_auc_score(mask(data_2)['is_signal'], pred_2, sample_weight=problem_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define correspondence matrix separately for even/odd, use several radii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "default = [[31.5, 34, 34.5, 2]]\n",
    "submatrices = []\n",
    "\n",
    "# Define even and odd layer wires\n",
    "even_wires = hits.cydet.point_pol != 1\n",
    "odd_wires = hits.cydet.point_pol == 1\n",
    "\n",
    "# TODO test with several radii\n",
    "for rmin, rmean, rmax, rsgma in default: #[[30, 33, 35], [26, 29, 32]]:\n",
    "    # Try with default values\n",
    "    hough = Hough(hits, sig_rho_min=rmin, sig_rho=rmean, sig_rho_max=rmax, sig_rho_sgma=rsgma)\n",
    "    corresp = hough.correspondence\n",
    "    \n",
    "    # Make even and odd layer hough matricies\n",
    "    corresp_odd = corresp.copy()\n",
    "    corresp_odd[even_wires, :] = 0\n",
    "    corresp_even = corresp.copy()\n",
    "    corresp_even[odd_wires, :] = 0\n",
    "    \n",
    "    # Append the two matricies\n",
    "    submatrices.append(corresp_odd)\n",
    "    submatrices.append(corresp_even)\n",
    "\n",
    "# Stack all submatricies horizontally\n",
    "new_correspondence = sparse.hstack(submatrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4482, 5634), 264565)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape, and the number of non-zero entries\n",
    "new_correspondence.shape, new_correspondence.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_max(data, neighs_matrix, alpha=10):\n",
    "    \"\"\"\n",
    "    This is soft version of 'is_max' rule.\n",
    "    Strong version returns 1 it it is max among neighbors, 0 otherwise.\n",
    "    The greater alpha, the closer we are to 'strong' version\n",
    "    \n",
    "    :param data:          Hough image(s) from transformation(s)\n",
    "    :param neighs_matrix: Nearest neighbour matrix for hough track-centres\n",
    "    :param alpha:         Weight of exponential reweighting \n",
    "    \"\"\"\n",
    "    # Exponentially reweight the data\n",
    "    exponents = numpy.exp(alpha * data)\n",
    "    # Check the number of maxima we expect to get back\n",
    "    # Note, we expect two maxima for each tested hough transform\n",
    "    # One for even, one for odd.  \n",
    "    # Testing multiple transforms scales as 2*n_transforms\n",
    "    n_parts = data.shape[1] // neighs_matrix.shape[1]\n",
    "    assert n_parts * neighs_matrix.shape[1] == data.shape[1]\n",
    "    # Block diagnol matrix, with each block being one copy of the neighs_matrix\n",
    "    full_neigh = sparse.block_diag([neighs_matrix] * n_parts, format='csr')\n",
    "    # Return the value at the point\n",
    "    # normalized the sum of its values and its neighbouring values\n",
    "    return exponents / full_neigh.dot(exponents.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalized nearest neighbors matrix for tracks\n",
    "## Pick out the neighbours\n",
    "nns = hough.track.point_neighbours\n",
    "## Add a track centre as its own neighbour\n",
    "nns = nns + sparse.identity(nns.shape[1])\n",
    "## Extend the neighbours out one to the left and one to the right\n",
    "## Now a 3x5 block\n",
    "nns = nns.dot(hough.track.lr_neighbours)\n",
    "## Weight the closer neighbours as double the further onesca\n",
    "nns += hough.track.lr_neighbours \n",
    "## Normalize \n",
    "nns = sparse.csr_matrix(nns / nns.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since hough has some parameters,  those should be fitted on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HoughTransform(object):\n",
    "    def __init__(self, hough_matrix, track_nns, min_percentile=0.97, alpha=2):\n",
    "        \"\"\"\n",
    "        Define a hough transform class that we can fit to the data\n",
    "        \n",
    "        :param hough_matrix:    Hough transform matrix\n",
    "        :param track_neighs:    Nearest neighbour matrix for tracks\n",
    "        :param min_percentile:  Bottom percentile ignored in hough space \n",
    "        :param alpha:           Weight of reweight in hough space\n",
    "        \"\"\"\n",
    "        self.hough_matrix = hough_matrix\n",
    "        self.min_percentile = min_percentile\n",
    "        self.alpha = alpha\n",
    "        # Normalize the hough transform so to decrease bias towards track-centre layers \n",
    "        # with better coverage, i.e. more wires in range\n",
    "        self.normed_corresp = sparse.csr_matrix(hough_matrix / (hough_matrix.sum(axis=1) + 50.))\n",
    "        self.track_nns = track_nns\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the hough transform to the data\n",
    "        \n",
    "        :param X: Data to be fit\n",
    "        \"\"\"\n",
    "        #self.n_features = X.shape[1]\n",
    "        \n",
    "        # Center the input distribution around 0\n",
    "        self.original_mean = X.mean()\n",
    "        original = X - self.original_mean\n",
    "\n",
    "        # Transform into hough space\n",
    "        hough_images = self.normed_corresp.T.dot(original.T).T\n",
    "        \n",
    "        # Use a percentile binning with increased sampling near 1\n",
    "        x = numpy.linspace(0, 1, 200) ** 0.5\n",
    "        # Get the percentile distribution\n",
    "        self.percentiles = numpy.percentile(hough_images.flatten(), x * 100)\n",
    "        # Remove the bottom min_percentile and shift the remaining range to fit in [0-1]\n",
    "        self.values_at_percentiles = numpy.maximum(0., x - self.min_percentile) / (1. - self.min_percentile)\n",
    "        hough_images = numpy.interp(hough_images, self.percentiles, self.values_at_percentiles)\n",
    "        \n",
    "        # Sharpen locally maximum peaks\n",
    "        hough_images = hough_images * is_max(hough_images, neighs_matrix=self.track_nns, alpha=self.alpha)\n",
    "        # Exponentiate the \n",
    "        hough_images = numpy.exp(self.alpha * hough_images)\n",
    "        self.mean_hough = hough_images.mean()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data according to the fit\n",
    "        \n",
    "        :param X: Data to be transformed\n",
    "        \n",
    "        :return after_hough, hough_images: ???\n",
    "        \"\"\"\n",
    "        # Center the input distribution around 0\n",
    "        original = X - self.original_mean\n",
    "        \n",
    "        # Perform the hough transform\n",
    "        hough_images = self.normed_corresp.T.dot(original.T).T\n",
    "        # Remove the bottom min_percentile and shift the remaining range to fit in [0-1]\n",
    "        hough_images = numpy.interp(hough_images, self.percentiles, self.values_at_percentiles)        \n",
    "        \n",
    "        # Reweight hough images by how locally maximal each point is\n",
    "        hough_images = hough_images * is_max(hough_images, neighs_matrix=self.track_nns, alpha=self.alpha)\n",
    "        # Reweight result exponentially\n",
    "        hough_images = numpy.exp(self.alpha * hough_images)\n",
    "        # Center around 0\n",
    "        hough_images -= self.mean_hough\n",
    "        \n",
    "        # Inverse hough transform\n",
    "        after_hough = self.normed_corresp.dot(hough_images.T).T\n",
    "        \n",
    "        return after_hough, hough_images    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning second classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.HoughTransform at 0x1f86e490>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform the second data sample\n",
    "hough_transformer = HoughTransform(hough_matrix=new_correspondence, track_nns=nns)\n",
    "hough_transformer.fit(pred_evt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_3, pred_evt_3 = predict_classifier(neigh_gbdt, data_3)\n",
    "\n",
    "# Get the hough results\n",
    "hough_result_2, hough_image_2 = hough_transformer.transform(pred_evt_2)\n",
    "hough_result_3, hough_image_3 = hough_transformer.transform(pred_evt_3)\n",
    "\n",
    "# Add these as features\n",
    "data_2['Hough'] = hough_result_2.flatten()\n",
    "data_3['Hough'] = hough_result_3.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add this to the feature list\n",
    "train_features_full = train_features + ['Hough']\n",
    "\n",
    "# Define the new classifier\n",
    "hough_gbdt = SklearnClassifier(GradientBoostingClassifier(n_estimators=100), \n",
    "                               features=train_features_full) \n",
    "\n",
    "# Define GBDT over only energy deposit to compare\n",
    "dep_gbdt = SklearnClassifier(GradientBoostingClassifier(n_estimators=200), \n",
    "                               features=['deposit']) \n",
    "\n",
    "# Ensure neigh-level GBDT has enough trees to compare properly\n",
    "neigh_gbdt.set_params(n_estimators=200)\n",
    "\n",
    "# Add the classifiers\n",
    "factory.add_classifier('Hough, Local and Neighbour Features', hough_gbdt)\n",
    "factory.add_classifier('Energy Deposition', dep_gbdt)\n",
    "factory['Local and Neighbour Features'] = neigh_gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "factory.fit(mask(data_2), mask(data_2)['is_signal'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test it on the last set of data\n",
    "pred_final_diag_3 = factory.test_on(mask(data_3), mask(data_3)['is_signal'])\n",
    "pred_final_3, pred_final_evt_3 = predict_classifier(hough_gbdt, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figsize(6.4,6.4) \n",
    "hough_level = factory[\"Hough, Local and Neighbour Features\"]\n",
    "feat_import = hough_level.get_feature_importances()\n",
    "feat_import = feat_import.sort(\"effect\", ascending=False)\n",
    "feat_label_dict = OrderedDict([\n",
    "        ('l_rel_time' , \"Left Timing\"),  \n",
    "        ('r_rel_time' , \"Right Timing\"),\n",
    "        (\"layer_id\" , \"R Distance\"), \n",
    "        ('r_deposit' , 'Right Energy'), \n",
    "        ('l_deposit' , 'Left Energy'), \n",
    "        ('rel_time' , \"Timing\"),\n",
    "        ('deposit' , 'Energy'), \n",
    "        ('Hough' , r\"Hough, $W_j''$\")\n",
    "    ])\n",
    "ax = plt.subplot(111)\n",
    "ax.bar( np.arange(len(feat_import.values)), feat_import.values,zorder=10)\n",
    "ax.set_xticks(np.arange(len(feat_import.values))+0.5)\n",
    "ax.minorticks_on()\n",
    "ax.set_ylabel(\"Normalized Relative Usage\")\n",
    "ax.set_title(\"Feature Importance\")\n",
    "ax.autoscale()\n",
    "\n",
    "ax.grid(b=True, which='major', axis='y' ,color='grey', linestyle='--')\n",
    "ax.grid(b=True, which='minor', axis='y' , color='grey', linestyle=':')\n",
    "ax.set_xticklabels(feat_label_dict.values()[::-1],  rotation='vertical')\n",
    "ax.set_yticklabels([\"{:.0f}%\".format(tick*100) for tick in ax.get_yticks()])\n",
    "show()\n",
    "\n",
    "corr = pred_final_diag_3.features_correlation_matrix(features=train_features+[\"Hough\"],\n",
    "                            tick_labels=[feat_label_dict[key] for key in train_features_full])\n",
    "corr.fontsize = 15\n",
    "corr.cmap = \"RdBu\"\n",
    "corr.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_feature( (data_3['Hough'].values+0.1), data_3['labels'].values, \n",
    "             xlabel=\"Hough Output\", ylabel=\"Normalised Hit Count\", xlog=True,\n",
    "            title=\"Reweighted Inverse Hough Output Distribution\", nbins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Final quality of signal vs bck wires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print roc_auc_score(mask(data_3)['is_signal'], pred_final_3)\n",
    "figsize(11,6) \n",
    "plot_rocs(mask(data_3)[\"is_signal\"], pred_final_diag_3.prediction, zoom=False)\n",
    "plt.savefig(\"/home/elg112/COMET/Presentations_Papers/group_meetings/images/sklearn_roc.png\", bbox_inches=\"tight\")\n",
    "show()\n",
    "plot_rocs(mask(data_3)[\"is_signal\"], pred_final_diag_3.prediction, zoom=True)\n",
    "plt.savefig(\"/home/elg112/COMET/Presentations_Papers/group_meetings/images/sklearn_roc_zoom.png\", bbox_inches=\"tight\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "problem_weights_3 = (mask(data_3)['labels'] != 0) * abs(mask(data_3)['is_signal'] - pred_3) ** 2\n",
    "print roc_auc_score(mask(data_3)['is_signal'], pred_3)\n",
    "print roc_auc_score(mask(data_3)['is_signal'], pred_3, sample_weight=problem_weights_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "problem_weights_3 = (mask(data_3)['labels'] != 0) * abs(mask(data_3)['is_signal'] - pred_final_3) ** 2\n",
    "print roc_auc_score(mask(data_3)['is_signal'], pred_final_3)\n",
    "print roc_auc_score(mask(data_3)['is_signal'], pred_final_3, sample_weight=problem_weights_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Progress of Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#optimize hough to improve track purity, use RANSAC to shed background islands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evt_hits = event_to_hit_look[event]\n",
    "to_plot = data_3[\"labels\"].values[evt_hits]\n",
    "hough_output = data_3[\"Hough\"].values[evt_hits]\n",
    "plot_hough_image_3 = hough_image_3[event,:hough.track.n_points] + hough_image_3[event,hough.track.n_points:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Basic output\n",
    "plot_output(to_plot, hits.cydet)\n",
    "show()\n",
    "# First GBDT output\n",
    "plot_output(to_plot, hits.cydet, size=plot_norm_size(pred_evt_3[event,:]))\n",
    "plot_add_outlines(to_plot, hits.cydet)\n",
    "show()\n",
    "# Hough output\n",
    "plot_output(to_plot, hits.cydet, size=plot_norm_size(pred_evt_3[event,:]), add_transform=True)\n",
    "plot_add_tracks(hits.cydet, hough, size=plot_norm_size(plot_hough_image_3,40))\n",
    "show()\n",
    "# Inverse Hough output\n",
    "plot_output(to_plot, hits.cydet, size=plot_norm_size(hough_output))\n",
    "plot_add_tracks(hits.cydet, hough, size=plot_norm_size(plot_hough_image_3,40), add_transform=True,\n",
    "                tolerance=0.00)\n",
    "show()\n",
    "# Final\n",
    "plot_output(to_plot, hits.cydet, size=plot_norm_size(pred_final_evt_3[event,:]))\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write a python generator that yields (like an interator)\n",
    "# class inherets from icometeventloop \n",
    "# wrap the event as python object\n",
    "# how PyROOT or rootpy do this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
